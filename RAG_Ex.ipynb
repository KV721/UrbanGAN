{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KV721/UrbanGAN/blob/main/RAG_Ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkuA7tkmbklb"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub transformers sentence-transformers\n",
        "!pip install docling\n",
        "!pip install chromadb\n",
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.chunking import HybridChunker\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from google import genai"
      ],
      "metadata": {
        "id": "H8rve9TNboSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GEMINI_API_KEY'] =\"\""
      ],
      "metadata": {
        "id": "-lg2Qo-EbtST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = DocumentConverter()\n",
        "document_path = \"/content/set_transfomer.pdf\"\n",
        "\n",
        "result = converter.convert(document_path)\n",
        "full_text = result.document.export_to_markdown()"
      ],
      "metadata": {
        "id": "hUUAkcDBbxLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸŸ¢ TODO 2: Complete the simple chunking function.\n",
        "# Split `text` into overlapping chunks of size `chunk_size`\n",
        "# with `overlap` characters shared between consecutive chunks.\n",
        "def chunk_text(text, chunk_size, overlap):\n",
        "    \"\"\"Split text into chunks with overlap.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_length = len(text)\n",
        "\n",
        "    while start < text_length:\n",
        "        end = start + chunk_size\n",
        "        # TODO: Append the chunk from `start` to `end`\n",
        "\n",
        "        # TODO: Move `start` forward by (chunk_size - overlap)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "simple_chunks = chunk_text(full_text, chunk_size=256, overlap=50)\n",
        "print(f\"âœ“ Simple chunks created: {len(simple_chunks)}\\n\")\n",
        "\n",
        "for i, chunk in enumerate(simple_chunks[:3]):\n",
        "    print(f\"\\n--- Simple Chunk {i+1} ---\")\n",
        "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
      ],
      "metadata": {
        "id": "5eWcPp82b0vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸŸ¡ TODO 3: Initialize the HybridChunker with the correct parameters.\n",
        "# Use:\n",
        "#   tokenizer=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#   max_tokens=256\n",
        "#   merge_peers=False\n",
        "chunker = HybridChunker(\n",
        "    # TODO: fill in the three parameters\n",
        ")\n",
        "\n",
        "chunk_iter = chunker.chunk(dl_doc=result.document)\n",
        "hybrid_chunks = list(chunk_iter)\n",
        "\n",
        "print(f\"âœ“ Hybrid chunks created: {len(hybrid_chunks)}\\n\")\n",
        "\n",
        "for i, chunk in enumerate(hybrid_chunks[:3]):\n",
        "    print(f\"\\n--- Hybrid Chunk {i+1} ---\")\n",
        "    chunk_text_str = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
        "    print(f\"Content: {chunk_text_str[:200]}...\" if len(chunk_text_str) > 200 else chunk_text_str)"
      ],
      "metadata": {
        "id": "P1UtE76CcBY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_chunks_text = [\n",
        "    chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
        "    for chunk in hybrid_chunks\n",
        "]\n",
        "\n",
        "print(f\"âœ“ Total simple chunks (for comparison): {len(simple_chunks)}\")\n",
        "print(f\"âœ“ Total hybrid chunks: {len(hybrid_chunks_text)}\")"
      ],
      "metadata": {
        "id": "CTZ03hYfcRBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# ðŸŸ¡ TODO 4: Load the SentenceTransformer model using EMBEDDING_MODEL_NAME\n",
        "# Hint: use SentenceTransformer(...)\n",
        "embedding_model = # TODO\n",
        "\n",
        "# Test it\n",
        "test_text = \"This is a test sentence for embeddings.\"\n",
        "test_embedding = embedding_model.encode(test_text)\n",
        "\n",
        "print(f\"âœ“ Embedding dimension: {len(test_embedding)}\")\n",
        "print(f\"âœ“ Sample embedding (first 2 values): {test_embedding[:2]}\")"
      ],
      "metadata": {
        "id": "pja8QJ6KcRyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=EMBEDDING_MODEL_NAME\n",
        ")\n",
        "\n",
        "collection_name = \"rag_documents\"\n",
        "\n",
        "try:\n",
        "    chroma_client.delete_collection(name=collection_name)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ðŸŸ¡ TODO 5: Create a new ChromaDB collection.\n",
        "# Use chroma_client.create_collection(...) with parameters:\n",
        "#   - name=collection_name\n",
        "#   - embedding_function=sentence_transformer_ef\n",
        "collection = # TODO"
      ],
      "metadata": {
        "id": "FzwQYDdscY3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸŸ¡ TODO 6: Prepare IDs and metadata, then add documents to the collection.\n",
        "# Each chunk needs a unique ID like \"chunk_0\", \"chunk_1\", etc.\n",
        "# Metadata should include chunk_index, source, and chunking_method.\n",
        "\n",
        "ids = [f\"chunk_{i}\" for i in range(len(hybrid_chunks_text))]\n",
        "metadatas = [{\"chunk_index\": i, \"source\": document_path, \"chunking_method\": \"hybrid\"} for i in range(len(hybrid_chunks_text))]\n",
        "\n",
        "print(\"Adding documents to ChromaDB...\")\n",
        "\n",
        "# TODO: Use collection.add(...) with documents, ids, and metadatas\n",
        "collection.add(\n",
        "    # TODO: fill in the two arguments\n",
        "    documents=hybrid_chunks_text\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Successfully stored {len(hybrid_chunks_text)} chunks in ChromaDB\")\n",
        "print(f\"âœ“ Collection now contains {collection.count()} documents\")"
      ],
      "metadata": {
        "id": "rptxdlUtch6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸŸ¡ TODO 7: Implement the search function using ChromaDB's query method.\n",
        "def search_documents(query, n_results=3):\n",
        "    \"\"\"Search the vector store for chunks most relevant to the query.\"\"\"\n",
        "    # TODO: Use collection.query(...) to search\n",
        "    # Hint: pass query_texts=[query] and n_results=n_results\n",
        "    results = # TODO\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "J-dBLJZkcyzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client()\n",
        "\n",
        "test_response = client.models.generate_content(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    contents=\"Explain how AI works in a few words\"\n",
        ")\n",
        "print(\"Gemini Flash Model Test:\")\n",
        "print(test_response.text)\n",
        "print(\"\\nâœ“ Gemini client initialized successfully!\")"
      ],
      "metadata": {
        "id": "nHPohUWYc3wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”´ TODO 8: Build the full RAG pipeline function.\n",
        "# This is the capstone â€” tie retrieval, augmentation, and generation together.\n",
        "def rag_query(question, n_results=3):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline:\n",
        "    1. Retrieve relevant documents\n",
        "    2. Augment the prompt with retrieved context\n",
        "    3. Generate answer using Gemini\n",
        "    \"\"\"\n",
        "    print(f\"Question: {question}\\n\")\n",
        "\n",
        "    # Step 1: Retrieve relevant documents\n",
        "    # TODO: Call search_documents function with the question\n",
        "    search_results = # TODO\n",
        "\n",
        "    retrieved_docs = search_results['documents'][0]\n",
        "    print(f\"âœ“ Retrieved {len(retrieved_docs)} relevant chunks\\n\")\n",
        "\n",
        "    # Step 2: Build context string from retrieved documents\n",
        "    # TODO: Join the retrieved docs into a single context string.\n",
        "    # Format each as \"[Context i]\\n<doc text>\" separated by double newlines.\n",
        "    context = # TODO\n",
        "\n",
        "    # Step 3: Create the augmented prompt\n",
        "    # ðŸ”´ TODO 9 (Prompt Engineering): Write a prompt that instructs the model to:\n",
        "    #   - Answer based ONLY on the provided context\n",
        "    #   - Admit when the context is insufficient\n",
        "    #   - Be clear and concise\n",
        "    prompt = f\"\"\"\n",
        "    # TODO: Write your prompt here. Include {context} and {question}.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 4: Generate response\n",
        "    # TODO: Use client.models.generate_content(...) with model=\"gemini-3-flash-preview\"\n",
        "    response = # TODO\n",
        "\n",
        "    print(\"âœ“ Answer generated!\\n\")\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer\": response.text,\n",
        "        \"retrieved_chunks\": retrieved_docs,\n",
        "        \"distances\": search_results['distances'][0]\n",
        "    }"
      ],
      "metadata": {
        "id": "b57vTMFec6DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_questions = [\n",
        "    \"What is the main topic discussed in the document?\",\n",
        "    \"Can you summarize the key points?\",\n",
        "    \"What are the important details mentioned?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RAG SYSTEM DEMO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, question in enumerate(example_questions, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXAMPLE {i}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    result = rag_query(question, n_results=3)\n",
        "\n",
        "    print(\"ANSWER:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(result['answer'])\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(f\"\\nRETRIEVED CONTEXT CHUNKS:\")\n",
        "    for j, (chunk, distance) in enumerate(\n",
        "        zip(result['retrieved_chunks'], result['distances']), 1\n",
        "    ):\n",
        "        print(f\"\\nChunk {j} (Similarity Distance: {distance:.4f}):\")\n",
        "        print(f\"{chunk[:200]}...\")"
      ],
      "metadata": {
        "id": "hmrErmqodKFd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}